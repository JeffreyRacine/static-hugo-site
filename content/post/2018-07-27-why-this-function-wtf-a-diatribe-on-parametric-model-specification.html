---
title: Why This Function (WTF)? A diatribe on parametric model specification
author: Jeffrey S. Racine
date: '2018-07-27'
slug: why-this-function-wtf-a-diatribe-on-parametric-model-specification
categories: []
tags: []
header:
  caption: ''
  image: ''
---



<p>It is not uncommon to encounter practitioners who scribble down a parametric regression model and then proceed <em>as if their model had generated the data</em>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> No model selection exercise is attempted, model uncertainty is ignored, and statements are made about the underlying process that generated the data based solely on the model they scribbled down. That is, they proceed as if their ad hoc model is the <em>one true model</em> that faithfully mimics the unknown <em>data generating process</em> (DGP) and therefore could plausibly have generated the data. Any bridge that might otherwise connect the DGP to their model remains uncrossed. This is a conceit that betrays an excessively favourable opinion of the practitioner’s ability to divine the true nature of unknown relationships.</p>
<!-- To run the gauntlet is to take part in a form of corporal
punishment in which the party judged guilty is forced to run between
two rows of soldiers who strike out and attack them. -->
<p>Before proceeding further, a serious scientist engages in the process of model building and establishes that their parametric model runs the gauntlet of model diagnostics and is not likely to be at odds with the DGP. Serious scientists confront data because they are interested in learning about some aspect of the underlying DGP, not because they are entranced by the coefficients of some ad hoc model they scribbled down. Model uncertainty is acknowledged from the outset and a proper treatment of its existence is recognized to be the foundation upon which sound statistical inference rests. Analysis is based on data that is acknowledged to be generated by some unknown DGP, and not by some simplistic ad hoc parametric function; apparently, the two are easily confused.</p>
<p>Suppose that Mother Nature took a random draw from the real number line <span class="math inline">\(\mathbb{R}\)</span> (call this draw <span class="math inline">\(\theta\)</span>). Now suppose that we took a random draw from <span class="math inline">\(\mathbb{R}\)</span> (call this <span class="math inline">\(\theta^*\)</span>). The real number line is dense, and the likelihood that the two draws are equal is zero (i.e., <span class="math inline">\(\operatorname{Pr}(\theta^*=\theta)=0\)</span>). We teach students in introductory statistics that this is a measure zero event and is why we assign zero probability mass to the event. It would be a conceit for us to presume or assert that <span class="math inline">\(\theta^*=\theta\)</span>, and we would be wrong 100% of the time.</p>
<p>Suppose that Mother Nature drew a function at random from the space of, say, Lipschitz continuous functions <span class="math inline">\(\mathcal{F}\)</span> (call this <span class="math inline">\(\theta(x)\)</span>). Now suppose that we drew a function at random from <span class="math inline">\(\mathcal{F}\)</span> (call this <span class="math inline">\(\theta^*(x)\)</span>). The space <span class="math inline">\(\mathcal{F}\)</span> is dense, and obviously <span class="math inline">\(\operatorname{Pr}(\theta^*(x)=\theta(x))=0\)</span> almost everywhere (a.e.). It would be a conceit for us to presume or assert that <span class="math inline">\(\theta^*(x)=\theta(x)\)</span> a.e., and we would be wrong 100% of the time.</p>
<p>We conduct regression analysis because we are interested in learning about some unknown conditional mean function <span class="math inline">\(\operatorname{E}(Y|X=x)\)</span>, denoted by <span class="math inline">\(\theta(x)\)</span>. Presuming additive errors and strict exogeneity, then the unknown relationship of interest can be written as <span class="math inline">\(y=\theta(x)+\epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is a stochastic error term, <span class="math inline">\(y\)</span> is some outcome of interest, and <span class="math inline">\(x\)</span> is a vector of predictors. It is a conceit to presume or assert that, for any function we fancy writing down (call this <span class="math inline">\(\theta^*(x)\)</span>), the function just so happens to coincide with that for the underlying DGP, i.e., to assert that <span class="math inline">\(\theta^*(x)=\theta(x)\)</span> a.e. After all, the space of conditional mean functions is dense.</p>
<p>The statement that a fitted ad hoc parametric model (call this <span class="math inline">\(\hat\theta^*(x)\)</span>) such as the popular linear additive specification is <em>statistically consistent</em> is therefore most curious. For instance, people often state that the OLS/ML estimator <span class="math inline">\(\hat\beta=(X&#39;X)^{-1}X&#39;Y\)</span> is consistent for <span class="math inline">\(\beta\)</span> in the model <span class="math inline">\(X&#39;\beta\)</span>, i.e., that <span class="math inline">\(X&#39;\hat\beta=\hat\theta^*(x)\)</span> is consistent for <span class="math inline">\(X&#39;\beta=\theta^*(x)\)</span>. But the claim “a straight line fitted by the method of least squares is a consistent unbiased estimator of a straight line” is a far cry from the claim “a straight line fitted by the method of least squares is a consistent unbiased estimator of any unknown conditional mean function.” Since we are interested in the underlying DGP, what we in fact require is that <span class="math inline">\(X&#39;\hat\beta=\hat\theta^*(x)\)</span> is consistent for <span class="math inline">\(\theta(x)=\operatorname{E}(Y|X=x)\)</span>; this will only hold if <span class="math inline">\(\theta^*(x)=\theta(x)\)</span> which, as noted above, is a rather unlikely event.</p>
<p>There is no getting around the fact that any ad hoc underspecified model <span class="math inline">\(\hat\theta^*(x)\)</span> will be biased and inconsistent for <span class="math inline">\(\theta(x)\)</span>. Furthermore, the linear (in parameters and predictors) additive model devoid of interaction terms is the simplest and most underspecified model possible—it is a corner solution, a limiting case; the only model more underspecified is a constant function, which would deliver the unconditional mean rather than the conditional mean. Notwithstanding, it is the go-to model for a not inconsequential number of practitioners. To claim consistency for every unknown DGP encountered betrays an alarming lack of sophistication. Furthermore, since the practitioner has assumed a fixed model that does not depend on the sample size, this approach typically indicates that the estimate is more precise (i.e., less variable) than it really is. Consequently, conclusions supported by such models may in fact be unsupportable, which is particularly worrisome; the production of biased and inconsistent estimates that appear more precise than they actually are should not be given a free pass.</p>
<p>The not uncommon practice of writing down a linear additive model and sidestepping the important and non-trivial process of <em>model selection</em> is difficult to justify. The linear (in predictors and parameters) additive model lacking in interaction terms is a limiting and extreme case, a fiction that should be used solely for the purpose of teaching the principles of estimation and inference. Many have had it drilled into their heads that the presumption of linearity and additivity is for pedagogical purposes only and should not be used to describe <em>actual</em> relationships. Some apparently missed that lecture and use this naïve specification for all of their applied analysis regardless of the provenance of the data.</p>
<p>If a practitioner behaves as if any parametric model they scribble down is correctly specified, then they are asserting they know the <em>one true functional form</em> of some underlying unknown relationship. But if they know the true functional form, why do they not know the true parameter values as well? Both require the same degree of self-deception to my way of thinking (though I presume that they would be less likely to get away with the latter). So, the next time you are presented with results from a simple additive, linear (in parameters and predictors), no interactions regression model, it would behoove you to pose the following question to the presenter—“WTF?”</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>An easy <em>tell</em> is a parametric model that is additive, linear in parameters and predictors, and devoid of interaction terms.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
